{\rtf1\ansi\deff0\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset2 Symbol;}{\f2\fswiss\fprq2\fcharset0 Arial;}{\f3\froman\fprq2\fcharset128 Times New Roman;}{\f4\fnil\fprq2\fcharset0 Microsoft YaHei;}{\f5\fnil\fprq2\fcharset0 Mangal;}{\f6\fnil\fprq0\fcharset128 Mangal;}}
{\colortbl;\red0\green0\blue0;\red0\green0\blue128;\red128\green128\blue128;}
{\stylesheet{\s0\snext0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081 Default;}
{\*\cs15\snext15 Numbering Symbols;}
{\*\cs16\snext16\cf2\ul\ulc0\langfe255\lang255\lang255 Internet Link;}
{\s17\sbasedon0\snext18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb240\sa120\keepn\cf0\kerning1\hich\af4\langfe2052\dbch\af5\loch\f2\fs28\lang3081 Heading;}
{\s18\sbasedon0\snext18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081 Text body;}
{\s19\sbasedon18\snext19{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af6\loch\f0\fs24\lang3081 List;}
{\s20\sbasedon0\snext20{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb120\sa120\cf0\i\kerning1\hich\af0\langfe2052\dbch\af6\ai\loch\f0\fs24\lang3081 Caption;}
{\s21\sbasedon0\snext21{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af0\langfe2052\dbch\af6\loch\f0\fs24\lang3081 Index;}
}{\info{\creatim\yr0\mo0\dy0\hr0\min0}{\revtim\yr0\mo0\dy0\hr0\min0}{\printim\yr0\mo0\dy0\hr0\min0}{\comment OpenOffice}{\vern4100}}\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709\deftab709

{\*\pgdsctbl
{\pgdsc0\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1800\margrsxn1800\margtsxn1440\margbsxn1440\pgdscnxt0 Default;}}
\formshade{\*\pgdscno0}\paperh15840\paperw12240\margl1800\margr1800\margt1440\margb1440\sectd\sbknone\sectunlocked1\pgndec\pgwsxn12240\pghsxn15840\marglsxn1800\margrsxn1800\margtsxn1440\margbsxn1440\ftnbj\ftnstart1\ftnrstcont\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnrlc
\pgndec\pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
What is }{\i\ai\rtlch \ltrch\loch
wrong}{\rtlch \ltrch\loch
, with the Chinese Room?}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
To preface this whole diatribe, I want to make something clear. I ultimately disagree with John Searle's conclusion, which I take to be that it is possible for a machines to act, to observers (observing some form of output), }{\i\ai\rtlch \ltrch\loch
in every conceivable way}{\rtlch \ltrch\loch
 like a human being, and yet, despite this, not attain consciousness, due to some possible deficiency in what may be termed }{\i\ai\rtlch \ltrch\loch
'causal power'}{\rtlch \ltrch\loch
.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
My reasons for this will ultimately be explained. However, the main issue I want to address here is not specifically this conclusion and why I disagree with it (which I address in other articles), but the expression of the thought experiment itself and why I feel it is almost horrifically inadequate for this purpose. It is pretty clear, from looking at the form it takes, that the Chinese Room was expressedly created by Searle in order to specifically create the impression  that his conclusion\'97that machines might replicate behaviour without consciousness, and may in some sense be specifically incapable of consciousness, }{\b\ab\rtlch \ltrch\loch
not}{\rtlch \ltrch\loch
 due to what might be essentially called }{\i\ai\rtlch \ltrch\loch
structurally productive}{\rtlch \ltrch\loch
 properties [EXPAND ON THIS], but because of some aspect of their }{\i\ai\rtlch \ltrch\loch
constitution}{\rtlch \ltrch\loch
 (which is still, in some sense, an issue of structure, which I believe deserves further characterisation)\'97is the correct one. And it does this by throwing every means it can as making this seem reasonable, and alternate conclusions absurd.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
And here lies the problem I see with it. This problem is }{\b\ab\rtlch \ltrch\loch
not}{\rtlch \ltrch\loch
 that I disagree with the conclusion and hence feel that the very construction and goal of such a thought experiment is fundamentally flawed, but rather that I see real meaning and worth at what I believe that Searle is trying to do, yet feel that the Chinese Room fails at driving at the heart of the issue, and instead muddles what is at its core a rather subtle and delicate problem with a large amount of (to my mind) extraneous detail that serves to bias the intuition of the reader towards Searle's desired conclusion while dirtying the matter at heart.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
The Room conflates several issues; it seemingly implies (though not explicitly), that the form of processing being done by the man acting as processor is limited to a kind of }{\i\ai\rtlch \ltrch\loch
symbolic processing}{\rtlch \ltrch\loch
, where the rules are of a form such that they create a response by transforming the Chinese symbols into an English(?) reply by a finite set of rules that fit into what we might think of as a single ordinary room suitable for clerical work, and which involve minimal if any global state. The problem's statement doesn't explicitly specify this, and Searle later clarified that the room may potentially be taken as something with computational power at least equivalent to a continuously running Turing machine that could potentially simulate a brain, but the }{\i\ai\rtlch \ltrch\loch
impression}{\rtlch \ltrch\loch
 given by the statements make the practical realisation of something like it that was actually capable of doing so (simulating a brain) massively unintuitive to actually imagine in a practical form. This is a problem that contributes to the seemingly ridiculous nature of thinking such a system capable of possessing consciousness, which is a problem because I feel that }{\i\ai\rtlch \ltrch\loch
this aspect in particular }{\rtlch \ltrch\loch
(the shear logistics)}{\i\ai\rtlch \ltrch\loch
,}{\rtlch \ltrch\loch
 ought not to be a barrier to imagining this.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
The initial issue, that of the implications of }{\i\ai\rtlch \ltrch\loch
symbolic processing}{\rtlch \ltrch\loch
, through what would ultimately amount to something with computational power equivalent to a relatively simple finite state machine and a list of correspondence between inputs and outputs, is something that I do believe would have serious implications negative for the possibility of a machine, even one that partially simulates the output of an actual conscious being, being conscious itself. To clarify this, I will specify how an instance of such a machine might act. Such a machine, on receiving an input containing up to (N) }{\i\ai\rtlch \ltrch\loch
words or additional symbols, including pauses in the input that prompt output}{\rtlch \ltrch\loch
 (which will constitute an 'N-gram' of the input), has a lookup table that, on receiving such words, will compute a response based on looking up how to in its (potentially massive) table of candidate responses. On receiving less then (N) 'grams', the machine will respond to these (R) grams, and prompt for further responses, which may take up to (N \'96 R) further grams. This table may be 'one-to-one' in size compared to the size of the input taken to an exponential, hence looking up responses directly, or it may use more indirect methods of computing these responses, using grammatical transformations and logical or inferences etc. which might perform simple operations on the grammar (like converting questions into a more tractable form that can be more sensibly responded to if they meet certain pre-specified conditions, even if they would otherwise exceed the N token limit) or alter a relatively manageable global state (which might consider of several thousand tokens at most). But, either way, the fundamental method of this machine is to take utterances earlier provided by actual conscious beings and systematically recalling them or computed transformations of them in sensible ways such that they provide an appropriate response to the series of Chinese sentences. Such a machine may be able to pass a limited form of Turing test, given that in sufficiently short (less than N length) conversation the Room might 'keep track' of the conversation by treating the whole conversation as an N-gram, each input sentence of which would be responded to before further prompting. This form of computation I agree is likely not capable of conciousness, and certainly not the kind of conciousness that humans are capable of. Machines that are limited like this, where the range of responses are those which have been judged to be sensible by humans (or other conscious beings), as opposed to being entities that possess this ability to judge themselves and have the capacity to self reflect and reason about their own conscious experience, are likely going to be effectively dead at large.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
I contend however, that a machine (or Room) that was fully capable simulating most or all of the meaningful elements of neural activity in the brain (along with appropriate simulation of sensory and bodily awareness, which is an issue of its own) would necessarily be conscious and possess its own sense of awareness. This, I argue, follows on from the hypothesis that P-zombies are metaphysically impossible, and hence that any entity capable of dynamically reasoning about consciousness in a way that is structurally equivalent to an actual conciousness must itself be conscious.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
Human neural activity, in contrast to the operations of the symbol manipulating Room, are likely to be far more based on incredibly abstract relations between structures that add up into our sensible experience and the subconscious backing of it (which arbitrarily extends to any physical process that can affect it). [CLARIFY THIS]}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
But this still leads us to a fundamental problem, that of how the structure underlying consciousness might be defined in terms of causality. From Hume's inquiry into the issue of causality, we find that we have no way of being certain that cause and effect do actually result from immutable physical laws, even if in some sense this truly is the case. But disregarding that issue (and certainly it is an enormous issue if consciousness it to be seen an being absolutely dependent on causal power) we still have a more fundamental issue, which might be illustrated by a degenerate case.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
*In a sense, the very need for causal power to be a necessary factor of consciousness may well be one of the most solid metaphysical arguments in favour of the reality of causality.  The fact that a lack of causality would make it virtually impossible to reason about how consciousness might necessarily arise along with the physical (which would be crucial unless some form of substance dualism would be the case otherwise), makes hard causality a more reasonable proposition than it already it. We will still need to consider the consequences of 'non-strict' causality though.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
What if a random cloud of particles which physically interact with }{\i\ai\rtlch \ltrch\loch
each other}{\rtlch \ltrch\loch
 only minimally at best, just happened to be directed by outside forces such that they took on a form that was structurally the equivalent of a human mind and acted in every way as if it were like such a mind (in that }{\i\ai\rtlch \ltrch\loch
if}{\rtlch \ltrch\loch
 at any point it were to be converted into particles that interact the way that those which normally constitute a human do, it }{\i\ai\rtlch \ltrch\loch
would}{\rtlch \ltrch\loch
 be a human mind operating with ordinary causal power) }{\i\ai\rtlch \ltrch\loch
purely by chance}{\rtlch \ltrch\loch
. In other words, the forces influencing these particles are randomly influencing the individual particles chaotically, and it it only by a ridiculous amount of luck that they hold together in the form they do.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
*It should be noted that chaos beings of this sort are entirely different from Boltzman Brains, which I suspect are vastly more likely and hence (given enough space/time) common. Boltzman Brains are actually conscious entities that form sporadically out of chaotic interactions, as opposed to biogenesis producing sustainable, self-replicating life forms that eventually develop consciousness. They would not necessarily have to be self-aware, all that matters is that, minimally, some interaction occurs that causes qualia to be experienced in some form.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
This seems to offer a serious problem. It is not at all clear that this construct would have to be conscious. After all, in regards to causality and cybernetic power, nothing it does is actually under its own self control in any meaningful sense. So, even if P-zombies, creatures which are in all other ways identical to conscious beings but lacking conciousness, are a metaphysical absurdity, and we also grant this proposition extends to cases where something is }{\i\ai\rtlch \ltrch\loch
functionally}{\rtlch \ltrch\loch
 equivalent to another conscious being though different in its particular substrate, it still doesn't follow that this }{\i\b\ai\ab\rtlch \ltrch\loch
chaos being}{\rtlch \ltrch\loch
 is a conscious entity, even though it would in principle respond just like a conscious being, though entirely by chance occurrence. But this grants us a problem. Unless we also conjecture that chaos beings are concious, we could reason that their hypothetical non-consciousness could imply the non-consciousness of entities that in part rely on things outside their control in order for their thinking to take place.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
On some level, beings that are capable of what might be called 'self-control' in the cybernetic sense are subject to elements and events outside of their control. The principles of thermodynamics necessitate this in the large. All self-aware (and hence self-controlled) beings require external sources of energy, and are subject to all kinds of noise and chaotic events that can effect their operations. This may even extend down to a fundamental level of randomness due to chaotic events. However, it is clear that these fundamental conditions do not prevent consciousness from occurring as it does in humans, and it may be reasonable to conjecture that humans are constructed in a way that allows their process to effect causal relationships in a a way that out-grosses the sum of chaotic events that might otherwise prevent their effects. In other words, neural activity still forms meaningful patterns that do strongly affect the chances of specific consequences being determined by them, and their power to do this likely also enables (and necessitates) conscious processes.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
So while I can conjecture that humans are conscious and that hypothetical chaos beings probably are not, it is possible to come up with examples that are considerably more ambiguous. The sliced brain experiment is my attempt to express this, and in a way to }{\rtlch \ltrch\loch\loch\f3
rehabilitate a possible core insight of the Chinese Room.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch\loch\f3
In the sliced }
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081{\cf1\rtlch \ltrch\loch\loch\f3
On one hand, I will define a 'normal brain' as any physical-structure-in-time that creates a series of processes that are structurally equivalent to a chain-of-cause-and-effect that possesses or emulates (both of which are equivalent in my theory) the structure of a conscious mind. The human brain is naturally a 'normal brain', as is any computer simulation that exhaustively emulates the structure of a mind and its operation.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Conversely I will define a 'degenerate brain' as any, partial simulation that exhibits finite limitations of its behaviours, such that it's overall behaviour is equivalent to a finite lookup table with entries corresponding to a given input-set (where an input-set is a data structure containing the total of all the input that the 'brain' has received so far since a given defined start point). Such a brain does not really think, it is incapable of going through a process that would allow it to respond meaningfully to inputs outside of its predefined scope, and hence, I will claim, it will not be conscious, even though it may simulate the appearance of a consciousness to a finite extent (possibly through pre-recording and/or structural analysis of an actual mind capable of consciousness). A normal brain, though physically finite in itself, and hence technically constrained in behaviour to the extent that it occupies a limited part of space and time with the structures it has (which are in turn limited in resolution), is capable of levels of dynamic response (due to its substantial and dynamic internal state) far beyond the limitations of the degenerate brain. Where the normal brain is capable of extending thoughts and generalising dynamically, allowing novel thought processes, not such thing will occur in the degenerate brain.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Now, consider the 'sliced brain'. The sliced brain is not, as it were, an ordinary, functioning brain which happens to be cut up in space. Rather, it is the equivalent of a physical brain that is sliced }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
across time}{\cf1\rtlch \ltrch\loch\loch\f3
. An inert, preserved, intact mind with all of its functions preserved, is frozen in time (so that, potentially, if it were to be unfrozen, it could continue to function assuming its vital functions were supported). However, instead of being unfrozen, a measure of its potential activity, i.e. what it would do next in a given time slice were it unfrozen and certain inputs given to it, is calculated from knowledge of its structure. Then a }{\cf1\b\ab\rtlch \ltrch\loch\loch\f3
replication}{\cf1\rtlch \ltrch\loch\loch\f3
 of this mind is made, though altered so that it is constructed in accordance with the previously mentioned simulation calculations, and hence resembles what the mind }{\cf1\b\ab\rtlch \ltrch\loch\loch\f3
would}{\cf1\rtlch \ltrch\loch\loch\f3
 be like were it advanced a single 'step' or slice in time forward.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
The question then is, to what extend is this system operating with causal power and how, and how ought this affect the possibility and/or necessity of the resulting construct }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
being}{\cf1\rtlch \ltrch\loch\loch\f3
 according to a consciousness.*}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
*This expression I've made peculiar quite deliberately.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Now we can examine some of the basic consequences of this, and play around with it to explore different scenarios and how they might affect this mind. The first issue is that of what causes the replications? We might set up a machine to automatically perform a step-replication at a set interval, hence causing a sequence of brain states to accumulate over time and think via being scanned and having their resulting thought process simulated. In this case would it be meaningful to ask }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
where}{\cf1\rtlch \ltrch\loch\loch\f3
 the thought process occurs*, the brain or the computer?}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
*This is a legitimate question. There may be a sense in which it doesn\'81\'66t make sense to say that }{\cf1\b\ab\rtlch \ltrch\loch\loch\f3
consciousness}{\cf1\rtlch \ltrch\loch\loch\f3
 itself 'takes place' in a given space as opposed to another, as opposed to it simply }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
being}{\cf1\rtlch \ltrch\loch\loch\f3
 and having the experiences it does, which happen to be conditioned by the causal processes that it is constituted by and which might be said to take place in (what consciousnesses can regard as) specific locations in space-time.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
But there is naturally the possibility of a number of things happening that diverge from this, we could have it so that:}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
a) Other simulators go back to past brain states and continue on from them differently (providing different sensory input) in order to create their own branches. Is this like what happens to our own consciousness in Everret's quantum many worlds if they are our reality?}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
b) The machine may stop at any given point, or jumble up the simulation somehow. These possibilities may be similar to death or mental illness, but there is nothing stopping everything going back to normal at any point.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
c) A human may perform the replication process instead of a machine. This is interesting, as it is closer to the Chinese Room than any other scenario so far, and I feel hones in on a fundamental issue implicit in it. While the man may perform the replication \'81\'67by the book\'81\'68, all the steps are completely under his discretion, and at any time he could perform them differently, or not at all, causing a difference of thinking, or a cessation of it. In a way, this may be considered a kind of simplified version of the 'China Brain' [], except that instead of having a whole nation perform the actions of neurons, a single person simulates their actions and uses the self-performed simulation to create a new brain state. In both cases, the consequences of a neural action are things that are chosen to be performed by agents who are themselves fully capable of choosing to do otherwise when considered as part of the system.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
For (b) we might make an alternate supposition. What if, instead of having the sliced brain have the potential to be 'switched off' or suspended due to a sustaining force outside of its control happening to deactivate the continuous process which allows the consciousness to continue to remain embodied via the results of the processes that are possible because of it, we had, what are otherwise normal humans, whose chemical processes just happened to work in such a way that they also depend on some outside force to remain active, which can be turned off? Would this be substantially different in causal power? This should be seriously considered, as, if it is simply a matter of energy being supplied appropriately, then even a regular human could potentially be subject to such deprivation by having their brain potentially 'gated', or perhaps invaded by nano-machines, and hence the appropriate chemicals being prevented from accessing the parts they would need to remotely, causing a brain malfunction. Clearly ordinary intuition suggests that such a 'gated' brain, when functioning normally, should still be conscious, despite the }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
potential}{\cf1\rtlch \ltrch\loch\loch\f3
 being there for it to be easily deprived of normal functioning at any time. Clearly a simple dependence on outside sources of power in itself, or the possibility of being controlled by something else, isn't something that makes consciousness impossible. Even parts of our own brain that might be said to be unconscious have strong determining influence on our consciousness, and hence could in some sense be considered to be elements 'outside' of consciousness that control it.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
But given all this, the causal power within the brain could still be said to form a }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
self determining system,}{\cf1\rtlch \ltrch\loch\loch\f3
 i.e. a part of the brain cannot, under normal circumstances*, directly decide to stop supporting the system they are a part of as a whole. While in (c), such a thing is very much possible and we can easily imagine it happening. But then, if this possibility is substantial, then what would makes (c) so different to (b)? In (c), a seemingly separate Will is involved, while in (b), the whole system is automatic. And yet, the same fundamental processes occur in (b) and (c), with it being the case in both systems that the processes involve the same brain/mind states and simulation/extrapolation algorithms, with results that come about from causal chains that bring about a determined result, as opposed to being the result of happen-stance and random chaos.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Could it be that even chaos beings, with no formal causal power or even connection between their elements, which simply }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
appear}{\cf1\rtlch \ltrch\loch\loch\f3
 to be conscious, also not only will be, but }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
must}{\cf1\rtlch \ltrch\loch\loch\f3
 be? But then what is to say that every possible combination of occurrences couldn't be interpreted as some conscious 2-causing process, and hence it necessarily being the case that all possible consciousnesses are constantly arising due to everything. That would be the 2-effect of the chaos beings possessing feeling.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
*Certain monks may have been capable of extraordinary feats of mortality.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\f3
 }
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
In (c) it is again reasonable to ask }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
where}{\cf1\rtlch \ltrch\loch\loch\f3
 the thought occurs. Where is even the consciousness of our own mind? This is an especially interesting question to ask if we deny that the sense of place that is part of our egoic thought process is \'81\'67where our consciousness really is\'81\'68 [REFERENCE R. FEYNMAN'S DEPRIVATION CHAMPER EXPERIMENTS]. And if we so deny this, and follow the consequences, with any causal interaction that potentially affects what we feel potentially being part of the chain of events that actually gives rise to our minds, then there can be no place or matter that we can rule out as being \'81\'67not part of our consciousness\'81\'68. This is a clear and natural result that perfectly fits in with all kinds of non-dual and }{{\*\bkmkstart __DdeLink__0_343109992}\cf1\rtlch \ltrch\loch\loch\f3
panpsychist}{{\*\bkmkend __DdeLink__0_343109992}\cf1\rtlch \ltrch\loch\loch\f3
 thought, whether traditional or modern, but still leaves our investigation of consciousness with severe problems, as it fails to provide us with any model of how it is that specific experiences are conditioned by and arise out of specific causal intersections, if that is the case.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Let's put this into perspective. Going back to the original Chinese Room, lets looks at what the original thought experiment would imply, were we to take it as something that emulates the thought processes of an actual, Chinese speaking human mind. Such a Room would not only have to simulate a }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
bare}{\cf1\rtlch \ltrch\loch\loch\f3
 mind, but would also have to supply it with a simulated environment, a world, that let the emulated brain convincingly feel that it was human. If would not only need to be provided with 'basic' sense inputs, like eyesight, which have sensory information gathered at a specific point, but it would also need to simulate a whole body, which is capable of the expected response and feedback from muscles etc, in order for the emulated mind to make sense of things instead of going insane. This is not a trivial issue.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
---}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
Something that cannot be escaped is the 'reverse' issue of causality. While it is convenience to consider physical processes primary, and conscious experience secondary, when we say that we have decided to do something because we feel a certain way, our feelings are, in a very real sense, a cause of our actions. This is not to say that we would then have 'free-will' in the sense of an ability for consciousness to override physical causality, but at the very least some form of compatibalism ought to be necessary in order for our decision making processes to be 'real', as opposed to hanging on epiphenomena; as being merely epiphenomena would imply something like that, even if P-zombies still happen to not be possible, it would then effectively be a coincidence that our sense of wistfulness attaches itself to our actions in the way it does, as opposed to this arising out of a }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
cause}{\cf1\rtlch \ltrch\loch\loch\f3
* more fundamental.}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\rtlch \ltrch\loch
}
\par \pard\plain \s18{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb0\sa120\cf0\kerning1\hich\af0\langfe2052\dbch\af0\loch\f0\fs24\lang3081\sb0\sa0{\cf1\rtlch \ltrch\loch\loch\f3
*This type of cause is a metaphysical one, closer to a fundamental grounding that is tantamount to a platonic fundamental form as opposed to an event in space and time. I will refer to these as }{\cf1\i\ai\rtlch \ltrch\loch\loch\f3
2-causes}{\cf1\rtlch \ltrch\loch\loch\f3
.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
---\tab }
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
Another issue is the incomprehensibility of Chinese to the English speaking operator. It really should be recognised that this is irrelevant to the actual problem, and serves little purpose but misdirection from the core issue. Any operator of the sliced brain machine in the sliced brain experiment has no reason to be able to 'read' the brain function of the brain than the Room operator does. It does, in a sense, clarify part of the issue by making it clear that the Room operator is dealing with a set of symbols he doesn\'92t understand, and hence that he is manipulating them mechanically rather than using his own judgement to interpret them. But this still seems to conflate two different issues, and I feel it mistakes something minor and arguably irrelevant with something more important. Let's say that the operator spoke English and dealt with English sentences. This would not prevent him from manipulating the symbols according to the mechanical rules he is tasked to. This might allow him to choose to substitute a given response with one of his own making, and the possibility of this is something that is interesting to consider. However the chief concern is whether the system as a whole, including the operator, is capable of an additional consciousness, rather than simply the operator alone. This issue has been discussed over and over in other commentary on the Room though.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\i\ai\rtlch \ltrch\loch
But the main point of the present argument is that no purely formal model will ever be sufficient by itself for intentionality because the formal properties are not by themselves constitutive of intentionality, and they have by themselves no causal powers except the power, when instantiated, to produce the next stage of the formalism when the machine is running.}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
[] }{{\field{\*\fldinst HYPERLINK "https://en.wikipedia.org/wiki/China_brain" }{\fldrslt \cf2\ul\ulc0\langfe255\lang255\lang255\rtlch \ltrch\loch
https://en.wikipedia.org/wiki/China_brain}}}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
---}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
}
\par \pard\plain \s0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\nowidctlpar\cf0\kerning1\hich\af0\langfe2052\dbch\af0\afs24\lang1081\loch\f0\fs24\lang3081{\rtlch \ltrch\loch
Consciousness must admit at least some form of probabilistic cause as its basis. Aggregating multiple 'noisy' or somewhat random processes, as long as they have some bias one way or another, can result in an arbitrarily stable signal (in the sense that the direction it it biased in will be consistently detectable) as long as enough source material is used. Hence, arbitrarily random processes can potentially be used to generate structures with arbitrarily high causal power as long as they are constructed such that they have some correlative bias.}
\par }